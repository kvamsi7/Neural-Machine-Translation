{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scratchpad",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kvamsi7/Neural-Machine-Translation/blob/main/scratchpad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAfZzGNk7nkw"
      },
      "source": [
        "import string\n",
        "import re\n",
        "from numpy import array,argmax,random,take \n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Embedding,LSTM,RepeatVector\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "pd.set_option('display.max_colwidth',20)"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbX3bTr6-Cb4"
      },
      "source": [
        "Data Gathering and Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3_tB-A17_FY",
        "outputId": "2629cf58-84f0-4dba-fd64-d4615a813c50"
      },
      "source": [
        "!!curl -O http://www.manythings.org/anki/fra-eng.zip\n",
        "!!unzip fra-eng.zip"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Archive:  fra-eng.zip',\n",
              " 'replace _about.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n',\n",
              " 'replace fra.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n']"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "EUoXty8S-OQA",
        "outputId": "d615a4b2-3649-4698-b98d-0a223e6bedb2"
      },
      "source": [
        "data_path = '/content/fra.txt'\n",
        "\n",
        "with open(data_path,encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "text[:100]"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Go.\\tVa !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)\\nGo.\\tMarche.'"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3O9ikYi5-inZ"
      },
      "source": [
        "# split the data into sequences\n",
        "\n",
        "def to_lines(text):\n",
        "  lines = text.strip().split('\\n')\n",
        "  sents = [line.split('\\t')[:-1] for line in lines]\n",
        "  return sents"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHriV6KlCguu",
        "outputId": "138239ff-e596-4c45-eb17-b5ab58f95dfd"
      },
      "source": [
        "eng_fra_text = to_lines(text)\n",
        "eng_fra_text = array(eng_fra_text)  # converting into array\n",
        "eng_fra_text[:10]"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['Go.', 'Va !'],\n",
              "       ['Go.', 'Marche.'],\n",
              "       ['Go.', 'Bouge !'],\n",
              "       ['Hi.', 'Salut !'],\n",
              "       ['Hi.', 'Salut.'],\n",
              "       ['Run!', 'Cours\\u202f!'],\n",
              "       ['Run!', 'Courez\\u202f!'],\n",
              "       ['Run!', 'Prenez vos jambes à vos cous !'],\n",
              "       ['Run!', 'File !'],\n",
              "       ['Run!', 'Filez !']], dtype='<U349')"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McbSedIeCjo8",
        "outputId": "bec063bf-cbaa-4bfb-a7aa-265439989e00"
      },
      "source": [
        "eng_fra_text.shape"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(190206, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGKL4SgIDK6O"
      },
      "source": [
        "# we have around 1.9 million records of data "
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZCGB2OrDj3D"
      },
      "source": [
        "Taking subset of the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GA3H6a9DQIi"
      },
      "source": [
        "eng_fra_subset = eng_fra_text[:90000,:] # using first 90000 rows only to reduce the training time"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRuJmogxDro7"
      },
      "source": [
        "Data Cleaning "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yv8ggYp3DtQo"
      },
      "source": [
        "# remove puncutation\n",
        "\n",
        "eng_fra_subset[:,0] = [s.translate(str.maketrans(\" \",\" \",string.punctuation)) for s in eng_fra_subset[:,0]]\n",
        "eng_fra_subset[:,1] = [s.translate(str.maketrans(\" \",\" \",string.punctuation)) for s in eng_fra_subset[:,1]]"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ofgnz8jEJLx",
        "outputId": "c97a95a7-d992-40e6-fd1b-6f90645646df"
      },
      "source": [
        "eng_fra_subset"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['Go', 'Va '],\n",
              "       ['Go', 'Marche'],\n",
              "       ['Go', 'Bouge '],\n",
              "       ...,\n",
              "       ['Could you solve the problem',\n",
              "        'Pourriezvous résoudre le problème\\u202f'],\n",
              "       ['Could you solve the problem',\n",
              "        'Pourraistu résoudre le problème\\u202f'],\n",
              "       ['Could you speak more slowly',\n",
              "        'Pouvezvous parler plus lentement\\u202f']], dtype='<U349')"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiOmpdI9Fg1L",
        "outputId": "0eefda20-f1fb-4be4-906b-6c700d2443b5"
      },
      "source": [
        "# convert text to lowercases\n",
        "for i in range(len(eng_fra_subset)):\n",
        "  eng_fra_subset[i,0] = eng_fra_subset[i,0].lower()\n",
        "  eng_fra_subset[i,1] = eng_fra_subset[i,1].lower()\n",
        "eng_fra_subset[:5]"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['go', 'va '],\n",
              "       ['go', 'marche'],\n",
              "       ['go', 'bouge '],\n",
              "       ['hi', 'salut '],\n",
              "       ['hi', 'salut']], dtype='<U349')"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRkLY9dwGZ-I"
      },
      "source": [
        "Text to Sequences Conversion (word to Index Mapping)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQm11VanGDf5"
      },
      "source": [
        "# function to build a tokenizer (to build the vocabulary)\n",
        "def tokenization(lines):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(lines)\n",
        "  return tokenizer"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sgi_xECLG6KD",
        "outputId": "1058d49d-2e4e-465c-dff4-5bfe6837b683"
      },
      "source": [
        "# prepare english tokenizer\n",
        "eng_tokenizer = tokenization(eng_fra_subset[:,0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "print(f'English Vocabulary size {eng_vocab_size}')\n",
        "\n",
        "# prepare french tokenizer\n",
        "fre_tokenizer = tokenization(eng_fra_subset[:,1])\n",
        "fre_vocab_size = len(fre_tokenizer.word_index) + 1\n",
        "print(f'French Vocabulary size {fre_vocab_size}')"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary size 8533\n",
            "French Vocabulary size 20225\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Otw9N_xXHWqm"
      },
      "source": [
        "eng_len = fre_len = 8 # each sentence should be with lenght 8"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtVu42rIIgws"
      },
      "source": [
        "# encode and pad sequences, padding to a max sentence length as mentiones as above\n",
        "\n",
        "def encode_sequences(tokenizer,length,lines):\n",
        "  # integer encode sequences\n",
        "  seq = tokenizer.texts_to_sequences(lines)\n",
        "  # pad sequences with 0 \n",
        "  seq = pad_sequences(seq,maxlen=length,padding='post')\n",
        "  return seq"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AS0py4wJJzHj"
      },
      "source": [
        "We will encode English Setences as input and French setences as target sequences. This has to be done for both train and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MO05lDrJe5G"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# X = eng_fra_subset[:,0]\n",
        "# y = eng_fra_subset[:,1]\n",
        "\n",
        "# splitting the data to train and test\n",
        "train,test  = train_test_split(eng_fra_subset,test_size = 0.2,random_state = 42)"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ugVNjtFJ32s"
      },
      "source": [
        "# prepare the training data\n",
        "trainX = encode_sequences(eng_tokenizer,eng_len,train[:,0])\n",
        "trainy = encode_sequences(fre_tokenizer,fre_len,train[:,1])\n",
        "\n",
        "# prepare the testing data\n",
        "testX = encode_sequences(eng_tokenizer,eng_len,test[:,0])\n",
        "testy = encode_sequences(fre_tokenizer,fre_len,test[:,1])"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36497FNHNtl7"
      },
      "source": [
        "Define our Seq2Seq model architecture:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0qvkF8zKpEH"
      },
      "source": [
        "# building the NMT model\n",
        "\n",
        "def define_model(in_vocab,out_vocab,in_timesteps,out_timesteps,units):\n",
        "  model = Sequential()\n",
        "  # encoder\n",
        "  model.add(Embedding(in_vocab,units,input_length=in_timesteps,mask_zero=True))\n",
        "  model.add(LSTM(units))\n",
        "  model.add(RepeatVector(out_timesteps))\n",
        "\n",
        "  # decoder\n",
        "  model.add(LSTM(units,return_sequences=True))\n",
        "  model.add(Dense(out_vocab,activation='softmax'))\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXnrnLqcOrKA"
      },
      "source": [
        "# we are using the RMSprop optimiser in this model as it is good choice when working with RNN networks.\n",
        "\n",
        "# model compilation\n",
        "\n",
        "model = define_model(eng_vocab_size,fre_vocab_size,eng_len,fre_len,512)\n",
        "rms = optimizers.RMSprop(learning_rate =0.001)\n",
        "model.compile(optimizer=rms,loss='sparse_categorical_crossentropy')"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBMhhwAFP9aD"
      },
      "source": [
        "Training step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUfqxVsRPZIB",
        "outputId": "ae16b4d7-945c-4823-e10b-cf205e1125b4"
      },
      "source": [
        "# params\n",
        "epochs = 100\n",
        "batch_size = 512\n",
        "validation_split = 0.2\n",
        "\n",
        "history = model.fit(trainX,trainy.reshape(trainy.shape[0],trainy.shape[1],1),\n",
        "                    epochs = epochs, \n",
        "                    batch_size = batch_size,\n",
        "                    validation_split = validation_split)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "113/113 [==============================] - 33s 246ms/step - loss: 4.3898 - val_loss: 4.0766\n",
            "Epoch 2/100\n",
            "113/113 [==============================] - 26s 234ms/step - loss: 3.7842 - val_loss: 3.7017\n",
            "Epoch 3/100\n",
            "113/113 [==============================] - 27s 235ms/step - loss: 3.5309 - val_loss: 3.4749\n",
            "Epoch 4/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 3.2715 - val_loss: 3.2682\n",
            "Epoch 5/100\n",
            "113/113 [==============================] - 27s 235ms/step - loss: 3.0537 - val_loss: 3.0891\n",
            "Epoch 6/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 2.8754 - val_loss: 2.9590\n",
            "Epoch 7/100\n",
            "113/113 [==============================] - 27s 237ms/step - loss: 2.7196 - val_loss: 2.8555\n",
            "Epoch 8/100\n",
            "113/113 [==============================] - 27s 237ms/step - loss: 2.5821 - val_loss: 2.7854\n",
            "Epoch 9/100\n",
            "113/113 [==============================] - 27s 237ms/step - loss: 2.4536 - val_loss: 2.6865\n",
            "Epoch 10/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 2.3310 - val_loss: 2.6537\n",
            "Epoch 11/100\n",
            "113/113 [==============================] - 27s 237ms/step - loss: 2.2161 - val_loss: 2.5412\n",
            "Epoch 12/100\n",
            "113/113 [==============================] - 27s 235ms/step - loss: 2.1058 - val_loss: 2.4759\n",
            "Epoch 13/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 1.9951 - val_loss: 2.4048\n",
            "Epoch 14/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 1.8911 - val_loss: 2.3487\n",
            "Epoch 15/100\n",
            "113/113 [==============================] - 27s 237ms/step - loss: 1.7911 - val_loss: 2.2945\n",
            "Epoch 16/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 1.6966 - val_loss: 2.2579\n",
            "Epoch 17/100\n",
            "113/113 [==============================] - 27s 237ms/step - loss: 1.6098 - val_loss: 2.2159\n",
            "Epoch 18/100\n",
            "113/113 [==============================] - 27s 237ms/step - loss: 1.5256 - val_loss: 2.1841\n",
            "Epoch 19/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 1.4455 - val_loss: 2.1580\n",
            "Epoch 20/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 1.3702 - val_loss: 2.1376\n",
            "Epoch 21/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 1.2971 - val_loss: 2.1034\n",
            "Epoch 22/100\n",
            "113/113 [==============================] - 27s 235ms/step - loss: 1.2281 - val_loss: 2.1079\n",
            "Epoch 23/100\n",
            "113/113 [==============================] - 27s 235ms/step - loss: 1.1627 - val_loss: 2.0945\n",
            "Epoch 24/100\n",
            "113/113 [==============================] - 26s 234ms/step - loss: 1.1032 - val_loss: 2.0782\n",
            "Epoch 25/100\n",
            "113/113 [==============================] - 26s 235ms/step - loss: 1.0440 - val_loss: 2.0628\n",
            "Epoch 26/100\n",
            "113/113 [==============================] - 27s 235ms/step - loss: 0.9906 - val_loss: 2.0626\n",
            "Epoch 27/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 0.9372 - val_loss: 2.0612\n",
            "Epoch 28/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 0.8886 - val_loss: 2.0599\n",
            "Epoch 29/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 0.8445 - val_loss: 2.0419\n",
            "Epoch 30/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 0.8008 - val_loss: 2.0444\n",
            "Epoch 31/100\n",
            "113/113 [==============================] - 27s 235ms/step - loss: 0.7594 - val_loss: 2.0610\n",
            "Epoch 32/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 0.7181 - val_loss: 2.0638\n",
            "Epoch 33/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 0.6825 - val_loss: 2.0878\n",
            "Epoch 34/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 0.6483 - val_loss: 2.0715\n",
            "Epoch 35/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 0.6150 - val_loss: 2.0708\n",
            "Epoch 36/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 0.5853 - val_loss: 2.0852\n",
            "Epoch 37/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 0.5561 - val_loss: 2.0985\n",
            "Epoch 38/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 0.5277 - val_loss: 2.1264\n",
            "Epoch 39/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 0.5027 - val_loss: 2.1175\n",
            "Epoch 40/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 0.4803 - val_loss: 2.1238\n",
            "Epoch 41/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 0.4583 - val_loss: 2.1556\n",
            "Epoch 42/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 0.4376 - val_loss: 2.1606\n",
            "Epoch 43/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 0.4209 - val_loss: 2.1640\n",
            "Epoch 44/100\n",
            "113/113 [==============================] - 27s 237ms/step - loss: 0.4049 - val_loss: 2.1865\n",
            "Epoch 45/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 0.3883 - val_loss: 2.1879\n",
            "Epoch 46/100\n",
            "113/113 [==============================] - 27s 235ms/step - loss: 0.3747 - val_loss: 2.2065\n",
            "Epoch 47/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 0.3609 - val_loss: 2.2134\n",
            "Epoch 48/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 0.3499 - val_loss: 2.2340\n",
            "Epoch 49/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 0.3389 - val_loss: 2.2433\n",
            "Epoch 50/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 0.3286 - val_loss: 2.2547\n",
            "Epoch 51/100\n",
            "113/113 [==============================] - 27s 237ms/step - loss: 0.3208 - val_loss: 2.2738\n",
            "Epoch 52/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 0.3123 - val_loss: 2.2870\n",
            "Epoch 53/100\n",
            "113/113 [==============================] - 27s 237ms/step - loss: 0.3047 - val_loss: 2.3024\n",
            "Epoch 54/100\n",
            "113/113 [==============================] - 27s 237ms/step - loss: 0.2986 - val_loss: 2.3203\n",
            "Epoch 55/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 0.2917 - val_loss: 2.3195\n",
            "Epoch 56/100\n",
            "113/113 [==============================] - 27s 235ms/step - loss: 0.2854 - val_loss: 2.3372\n",
            "Epoch 57/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 0.2808 - val_loss: 2.3385\n",
            "Epoch 58/100\n",
            "113/113 [==============================] - 27s 237ms/step - loss: 0.2756 - val_loss: 2.3564\n",
            "Epoch 59/100\n",
            "113/113 [==============================] - 27s 237ms/step - loss: 0.2715 - val_loss: 2.3585\n",
            "Epoch 60/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 0.2673 - val_loss: 2.3810\n",
            "Epoch 61/100\n",
            "113/113 [==============================] - 27s 235ms/step - loss: 0.2639 - val_loss: 2.3925\n",
            "Epoch 62/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 0.2605 - val_loss: 2.4123\n",
            "Epoch 63/100\n",
            "113/113 [==============================] - 27s 237ms/step - loss: 0.2572 - val_loss: 2.4070\n",
            "Epoch 64/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 0.2543 - val_loss: 2.4235\n",
            "Epoch 65/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 0.2516 - val_loss: 2.4321\n",
            "Epoch 66/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 0.2495 - val_loss: 2.4355\n",
            "Epoch 67/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 0.2470 - val_loss: 2.4412\n",
            "Epoch 68/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 0.2450 - val_loss: 2.4562\n",
            "Epoch 69/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 0.2422 - val_loss: 2.4651\n",
            "Epoch 70/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 0.2406 - val_loss: 2.4753\n",
            "Epoch 71/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 0.2394 - val_loss: 2.4831\n",
            "Epoch 72/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 0.2374 - val_loss: 2.4904\n",
            "Epoch 73/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 0.2364 - val_loss: 2.5089\n",
            "Epoch 74/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 0.2349 - val_loss: 2.5060\n",
            "Epoch 75/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 0.2328 - val_loss: 2.5234\n",
            "Epoch 76/100\n",
            "113/113 [==============================] - 27s 235ms/step - loss: 0.2318 - val_loss: 2.5197\n",
            "Epoch 77/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 0.2304 - val_loss: 2.5329\n",
            "Epoch 78/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 0.2294 - val_loss: 2.5263\n",
            "Epoch 79/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 0.2286 - val_loss: 2.5448\n",
            "Epoch 80/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 0.2275 - val_loss: 2.5507\n",
            "Epoch 81/100\n",
            "113/113 [==============================] - 27s 237ms/step - loss: 0.2262 - val_loss: 2.5564\n",
            "Epoch 82/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 0.2249 - val_loss: 2.5679\n",
            "Epoch 83/100\n",
            "113/113 [==============================] - 27s 237ms/step - loss: 0.2243 - val_loss: 2.5820\n",
            "Epoch 84/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 0.2230 - val_loss: 2.5904\n",
            "Epoch 85/100\n",
            "113/113 [==============================] - 27s 237ms/step - loss: 0.2222 - val_loss: 2.5836\n",
            "Epoch 86/100\n",
            "113/113 [==============================] - 27s 236ms/step - loss: 0.2217 - val_loss: 2.5977\n",
            "Epoch 87/100\n",
            "113/113 [==============================] - 27s 237ms/step - loss: 0.2213 - val_loss: 2.6070\n",
            "Epoch 88/100\n",
            "113/113 [==============================] - 27s 238ms/step - loss: 0.2203 - val_loss: 2.6144\n",
            "Epoch 89/100\n",
            "113/113 [==============================] - 27s 237ms/step - loss: 0.2197 - val_loss: 2.6029\n",
            "Epoch 90/100\n",
            "113/113 [==============================] - 27s 237ms/step - loss: 0.2186 - val_loss: 2.6102\n",
            "Epoch 91/100\n",
            "113/113 [==============================] - 27s 237ms/step - loss: 0.2178 - val_loss: 2.6217\n",
            "Epoch 92/100\n",
            "113/113 [==============================] - 27s 237ms/step - loss: 0.2171 - val_loss: 2.6299\n",
            "Epoch 93/100\n",
            "113/113 [==============================] - 27s 237ms/step - loss: 0.2169 - val_loss: 2.6319\n",
            "Epoch 94/100\n",
            "113/113 [==============================] - 27s 237ms/step - loss: 0.2157 - val_loss: 2.6269\n",
            "Epoch 95/100\n",
            "113/113 [==============================] - 27s 237ms/step - loss: 0.2159 - val_loss: 2.6465\n",
            "Epoch 96/100\n",
            "113/113 [==============================] - 27s 237ms/step - loss: 0.2149 - val_loss: 2.6528\n",
            "Epoch 97/100\n",
            "113/113 [==============================] - 27s 237ms/step - loss: 0.2140 - val_loss: 2.6560\n",
            "Epoch 98/100\n",
            "113/113 [==============================] - 27s 238ms/step - loss: 0.2141 - val_loss: 2.6594\n",
            "Epoch 99/100\n",
            "113/113 [==============================] - 27s 237ms/step - loss: 0.2133 - val_loss: 2.6583\n",
            "Epoch 100/100\n",
            "113/113 [==============================] - 27s 237ms/step - loss: 0.2130 - val_loss: 2.6656\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5FoARt5QoFc"
      },
      "source": [
        "model.save('/content/seq2seq_nmt.h5')"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSGwkVa3Nr5q"
      },
      "source": [
        "# loading the model\n",
        "model = load_model('/content/seq2seq_nmt.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_tjB6Y_TIY2"
      },
      "source": [
        "# prediction\n",
        "\n",
        "y_prob = model.predict(testX.reshape(testX.shape[0],testX.shape[1])[:20])"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhSKW7ZqXtP8"
      },
      "source": [
        "preds = y_prob.argmax(axis=-1)"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVoMHtUZL_Fs"
      },
      "source": [
        "preds = y_prob.argmax(axis=-1)"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPqAHwyOVrkq",
        "outputId": "9a161bca-008c-4021-de28-5ae4f043bb6d"
      },
      "source": [
        "preds"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[    5,    38,  2504,   185,     2,     0,     0,     0],\n",
              "       [    5,  5641,     4,     0,     0,     0,     0,     0],\n",
              "       [   89,    21,  4066,     0,     0,     0,     0,     0],\n",
              "       [    1,     5,  1134,    11,   615,     0,     0,     0],\n",
              "       [   46,  1324,   387,     0,     0,     0,     0,     0],\n",
              "       [   18,  1081,     0,     0,     0,     0,     0,     0],\n",
              "       [   13,    13,    30,    21,    21,    21,    21,     0],\n",
              "       [  968,    56,    53,     0,     0,     0,     0,     0],\n",
              "       [    4,    15,    55,   150,   150,     0,     0,     0],\n",
              "       [    6,   115,     2,    25,     0,     0,     0,     0],\n",
              "       [    1,    76,     2,     2,     0,     0,     0,     0],\n",
              "       [    1,    76,    23,   215,    67,   138,     0,     0],\n",
              "       [  108,     7,  2310,     0,     0,     0,     0,     0],\n",
              "       [   72,  4513,  9978,     0,     0,     0,     0,     0],\n",
              "       [10729,    67,     1,    23,  2011,     0,     0,     0],\n",
              "       [   41,     8,   432,   289,     9,  1712,     0,     0],\n",
              "       [    1,    17,   405,     8,    20,   173,   399,     0],\n",
              "       [    4,    15,     2,   303,     0,     0,     0,     0],\n",
              "       [    1,   250,    12,    12,    12,    12,     0,     0],\n",
              "       [  122,   450,     9,    21,   359,     0,     0,     0]])"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPAtWjK93sGC"
      },
      "source": [
        "# these predictions are sequence of integers, \n",
        "\n",
        "def get_word(n,tokenizer):\n",
        "  for token,index in tokenizer.word_index.items():\n",
        "    # print(token)\n",
        "    if index == n:\n",
        "      return token\n",
        "  return"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ROpOLPP04JUH",
        "outputId": "2ee9824b-067b-43df-f471-385733ce34c4"
      },
      "source": [
        "get_word(432,fre_tokenizer)"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'estelle'"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0DDGSwE5wPu"
      },
      "source": [
        "preds_text = []\n",
        "\n",
        "for i in range(preds.shape[0]):\n",
        "  pred_= []\n",
        "  for j in range(preds.shape[1]):\n",
        "    val = preds[i,j]\n",
        "    if val > 0 :\n",
        "      word = get_word(preds[i,j],fre_tokenizer)\n",
        "      if word != None:\n",
        "        pred_.append(word)\n",
        "  preds_text.append(\" \".join(pred_))"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 665
        },
        "id": "uAJeiO_F526p",
        "outputId": "2944a2dd-4795-44d0-d1a2-4d8fc185ba39"
      },
      "source": [
        "eval_df = pd.DataFrame({'actual':test[:20,1],'predict':preds_text})\n",
        "eval_df"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>actual</th>\n",
              "      <th>predict</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tu es inquiet pa...</td>\n",
              "      <td>vous êtes inquiè...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tu aimerais tom</td>\n",
              "      <td>vous aimeriez tom</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>avezvous une que...</td>\n",
              "      <td>astu une question</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>je te laisserai ...</td>\n",
              "      <td>je vous laissera...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>pourquoi voudrie...</td>\n",
              "      <td>pourquoi voudrai...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>jai promis</td>\n",
              "      <td>jai promis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>nous nous réunis...</td>\n",
              "      <td>nous nous fait u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>vous y voici</td>\n",
              "      <td>ty y es</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>tom a été pris a...</td>\n",
              "      <td>tom a été par par</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>ne faites pas ça</td>\n",
              "      <td>ne fais pas ça</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>je nen avais pas...</td>\n",
              "      <td>je nai pas pas</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>le temps chaud n...</td>\n",
              "      <td>je nai me tellem...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>jadore les films...</td>\n",
              "      <td>jaime le chandail</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>estu un malade m...</td>\n",
              "      <td>êtesvous mentale...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>corrigemoi si je...</td>\n",
              "      <td>corrigezmoi si j...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>votre amie estel...</td>\n",
              "      <td>estce que estell...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>je suis content ...</td>\n",
              "      <td>je suis content ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>tom na pas faim</td>\n",
              "      <td>tom a pas faim</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>je plongeai dans...</td>\n",
              "      <td>je prends à à à à</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>lun est neuf lau...</td>\n",
              "      <td>mes prix est une...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 actual              predict\n",
              "0   tu es inquiet pa...  vous êtes inquiè...\n",
              "1       tu aimerais tom    vous aimeriez tom\n",
              "2   avezvous une que...   astu une question \n",
              "3   je te laisserai ...  je vous laissera...\n",
              "4   pourquoi voudrie...  pourquoi voudrai...\n",
              "5            jai promis           jai promis\n",
              "6   nous nous réunis...  nous nous fait u...\n",
              "7          vous y voici              ty y es\n",
              "8   tom a été pris a...    tom a été par par\n",
              "9     ne faites pas ça        ne fais pas ça\n",
              "10  je nen avais pas...       je nai pas pas\n",
              "11  le temps chaud n...  je nai me tellem...\n",
              "12  jadore les films...    jaime le chandail\n",
              "13  estu un malade m...  êtesvous mentale...\n",
              "14  corrigemoi si je...  corrigezmoi si j...\n",
              "15  votre amie estel...  estce que estell...\n",
              "16  je suis content ...  je suis content ...\n",
              "17      tom na pas faim       tom a pas faim\n",
              "18  je plongeai dans...    je prends à à à à\n",
              "19  lun est neuf lau...  mes prix est une..."
            ]
          },
          "metadata": {},
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gp8_c_9NPkfJ"
      },
      "source": [
        "Evaluating the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cj8UZPotRHiB"
      },
      "source": [
        "##### Lets Evaluate our model on basis of metric BLEU (Bilingual Evaluation Understudy Score)\n",
        "\n",
        "reference:  https://machinelearningmastery.com/calculate-bleu-score-for-text-python/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joT5VQpF_1yX"
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NmvAGIcPZkR"
      },
      "source": [
        "def get_bleu_score(reference,candidate):\n",
        "  reference = [[word.strip() for word in reference.split()]]\n",
        "  candidate = [word.strip() for word in candidate.split()]\n",
        "  return sentence_bleu(reference,candidate)"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CdvjWliRhZO",
        "outputId": "3ec48ae8-ab5e-43b9-9107-239e3e736fbc"
      },
      "source": [
        "# generate the bleu score\n",
        "\n",
        "scores = []\n",
        "for _,act,pred in eval_df.itertuples():\n",
        "  score_ = sentence_bleu(act,pred)\n",
        "  scores.append(score_)\n",
        "\n",
        "eval_df['BLEU_score'] = scores"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "M2ZzJbiOVOna",
        "outputId": "feb1319b-867b-4ca0-f27c-726c1e9601af"
      },
      "source": [
        "eval_df.head()"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>actual</th>\n",
              "      <th>predict</th>\n",
              "      <th>BLEU_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tu es inquiet pa...</td>\n",
              "      <td>vous êtes inquiè...</td>\n",
              "      <td>0.784781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tu aimerais tom</td>\n",
              "      <td>vous aimeriez tom</td>\n",
              "      <td>0.875765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>avezvous une que...</td>\n",
              "      <td>astu une question</td>\n",
              "      <td>0.884158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>je te laisserai ...</td>\n",
              "      <td>je vous laissera...</td>\n",
              "      <td>0.818251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>pourquoi voudrie...</td>\n",
              "      <td>pourquoi voudrai...</td>\n",
              "      <td>0.849891</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                actual              predict  BLEU_score\n",
              "0  tu es inquiet pa...  vous êtes inquiè...    0.784781\n",
              "1      tu aimerais tom    vous aimeriez tom    0.875765\n",
              "2  avezvous une que...   astu une question     0.884158\n",
              "3  je te laisserai ...  je vous laissera...    0.818251\n",
              "4  pourquoi voudrie...  pourquoi voudrai...    0.849891"
            ]
          },
          "metadata": {},
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ME1OCap1VPNl",
        "outputId": "8deae2a6-9ac3-47a5-faeb-0d416afc2762"
      },
      "source": [
        "print(f\"On an Avearage, we are getting BLEU score of {array(scores).mean():.2f} for our Neural Machine Translation model\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "On an Avearage, we are getting BLEU score of 0.83 for our Neural Machine Translation model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOXJs5tJYGDI"
      },
      "source": [
        "Approaches can imporove the model performance\n",
        "- We are using only first 90K rows cause of long training time, Training on more data can improve the model\n",
        "- using more sophisticated data cleaning approaches can also give more context\n",
        "- Adjusting the model architecture (ie: Attention model) etc will improve the model\n",
        "- Increasing the training time and adjusting the hyper parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnTYW9uKXKCD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}